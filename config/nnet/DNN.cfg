[nnet]
#name of the modeule holding the classifier
module = dnn
#name of the Classifier classifiers
class = DNN
#name of the neural net
name = DNN_5layers_2048units_relu_l2
# set to true for batch norm
batch_norm = False
#non linearity
nonlin = relu
# set to true for l2 norm
l2_norm = True
# dropout rate
dropout = 1
#number of layers
num_layers = 5
#number of units in each layer
num_units = 2048
#set to true for layerwise init
layerwise_init = False
#starting step, set to 'final' to skip nnet training
starting_step = 0
#number of passes over the entire database
num_epochs = 100
#initial learning rate of the neural net
initial_learning_rate = 1e-3
#exponential weight decay parameter
learning_rate_decay = 1
#size of the minibatch (#utterances)
batch_size = 8
#to limit memory ussage (specifically for GPU) the batch can be devided into
#even smaller batches. The gradient will be calculated by averaging the
#gradients of all these mini-batches. This value is the size of these
#mini-batches in number of utterances. For optimal speed this value should be
#set as high as possible without exeeding the memory. To use the entire batch
#set to -1
numutterances_per_minibatch = -1
#if there is no dev set q dev set will be creqted from the training set, this
#sets the number of training batches that will be used for validation
valid_utt = 16
#frequency of evaluating the validation set. CATION: If the valid_frequency is
#more than 5 times larger than the checkpoint frequency the validated model
#will be deleted by the saver
valid_frequency = 1000
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True
#number of times the learning will retry (with half learning rate) before
#terminating the training
valid_retries = 3
#how many steps are taken between two checkpoints
check_freq = 500
#you can visualise the progress of the neural net with tensorboard
visualise = True
#the beam width for the decoding
beam_width = 100
