[trainer]
trainer = fisher_trainer
#the wrapped section
wrapped = wrapped
#the weight of the fisher loss
fisher_weight = 1
#the data that should be used to compute the fisher information
fisher_features = devfbank
#whether the fisher information should be used for the encoder
fisher_encoder = True
#whether the fisher information should be used for the decoder
fisher_decoder = False
#the directory to store and retrieve the fisher information
fisher_dir = /users/spraak/vrenkens/spchtemp/Nabu/expdir/fisher/LAS/timit
#link the input names defined in the classifier config to sections defined in
#the database config
features = trainfbank
#a space seperated list of target names used by the trainer
targets = text
#a mapping between the target names and database sections
text = traintext
#number of passes over the entire database
num_epochs = 0
#initial learning rate of the neural net
initial_learning_rate = 1e-2
#exponential weight decay parameter
learning_rate_decay = 0.1
#size of the minibatch (#utterances)
batch_size = 128
#number of minibatches to aggregate before updating the parameters if 0
#asstnchronous training will be done
numbatches_to_aggregate = 0
#the data will be divided into buckets according to sequence length, this
#setting determines the number of buckets to use. For no bucketing set to 1
numbuckets = 16
# bool, change batch size from bucket to bucket, for buckets with higher
#seq_length a smaller batch size is used
variable_batch_size = True
#if set to True training will resume from latest checkpoint
resume_training = False

###VALIDATION PART###
#frequency of evaluating the validation set.
valid_frequency = 500
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True
#if you want to go back in training if validation performance is worse set to True
go_back = True
#the number of times validation performance can be worse before terminating training, set to None to disable early stopping
num_tries = 5
#set to True if you want to reset the number of tries if the validation performance is better
reset_tries = True


[decoder]
#name of the decoder that should be used
decoder = random_decoder
#the maximum number of output steps
max_steps = 100
#the alphabet used by the decoder
alphabet = sil aa ae ah aw ay b ch d dh dx eh er ey f g hh ih iy jh k l m n ng ow oy p r s sh t th uh uw v w y z


[wrapped]
#name of the trainer that should be used
trainer = eos_cross_entropy
